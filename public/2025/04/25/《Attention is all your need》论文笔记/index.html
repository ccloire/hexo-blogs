<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>《Attention is all your need》论文笔记 | Loire's Blog</title><meta name="author" content="Loire"><meta name="copyright" content="Loire"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文逐步分解读 论文网址： https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;attention-is-all-you-need https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762 Transformer模型论文摘要解读 这篇论文是著名的&quot;Attention Is All You Need&quot;论文的摘要，介绍了Transformer模型，这是深度学习和自然语"><meta property="og:type" content="website"><meta property="og:title" content="《Attention is all your need》论文笔记"><meta property="og:url" content="https://ccloire.com/2025/04/25/%E3%80%8AAttention%20is%20all%20your%20need%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html"><meta property="og:site_name" content="Loire&#39;s Blog"><meta property="og:description" content="论文逐步分解读 论文网址： https:&#x2F;&#x2F;paperswithcode.com&#x2F;paper&#x2F;attention-is-all-you-need https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762 Transformer模型论文摘要解读 这篇论文是著名的&quot;Attention Is All You Need&quot;论文的摘要，介绍了Transformer模型，这是深度学习和自然语"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://ccloire.com/img/butterfly-icon.png"><meta property="article:published_time" content="2025-04-25T05:10:05.000Z"><meta property="article:modified_time" content="2025-05-07T14:12:28.208Z"><meta property="article:author" content="Loire"><meta property="article:tag" content="多模态大模型论文学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://ccloire.com/img/butterfly-icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ccloire.com/2025/04/25/%E3%80%8AAttention%20is%20all%20your%20need%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"《Attention is all your need》论文笔记",isHighlightShrink:!1,isToc:!1,pageType:"posts"}</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Loire's Blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章发布时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-heartbeat"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-home-page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://caimotu.top/Picgo/微信图片_20250226100519.jpg" alt="Logo"><span class="site-name">Loire's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章发布时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-heartbeat"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="page-site-info"><h1 id="site-title">《Attention is all your need》论文笔记</h1></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="论文逐步分解读">论文逐步分解读</h1><p>论文网址：</p><p><a target="_blank" rel="noopener" href="https://paperswithcode.com/paper/attention-is-all-you-need">https://paperswithcode.com/paper/attention-is-all-you-need</a></p><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p><h2 id="transformer模型论文摘要解读">Transformer模型论文摘要解读</h2><p>这篇论文是著名的"Attention Is All You Need"论文的摘要，介绍了Transformer模型，这是深度学习和自然语言处理领域的一个里程碑式创新。以下是主要内容解读：</p><h3 id="研究背景与创新">研究背景与创新</h3><ul><li><p>之前的主流序列转导模型都基于复杂的循环神经网络(RNN)或卷积神经网络(CNN)，包含编码器和解码器结构</p></li><li><p>性能最好的模型还通过注意力机制连接编码器和解码器</p></li><li><p>论文提出了全新的网络架构"Transformer"，完全基于注意力机制，彻底摒弃了循环和卷积结构</p></li></ul><h3 id="实验结果">实验结果</h3><ul><li><p>在两个机器翻译任务上的实验表明，Transformer模型质量更优，同时更易于并行化，训练时间显著缩短</p></li><li><p>在WMT 2014英德翻译任务上达到28.4 BLEU分，比当时最好的结果(包括集成模型)高出2个BLEU以上</p></li><li><p>在WMT 2014英法翻译任务上，单模型达到41.8 BLEU的最新水平，仅用8个GPU训练3.5天，训练成本只是文献中最佳模型的一小部分</p></li></ul><h3 id="泛化能力">泛化能力</h3><ul><li>Transformer在其他任务上表现良好，成功应用于英语句法分析，无论是大规模还是有限的训练数据环境</li></ul><h2 id="研究背景">研究背景</h2><p>论文开篇指出：</p><ul><li><p>循环神经网络(RNN)特别是LSTM和GRU已成为序列建模和转导问题(如语言建模和机器翻译)的最先进方法</p></li><li><p>许多研究不断推动循环语言模型和编码器-解码器架构的边界</p></li></ul><h3 id="现有方法的局限性">现有方法的局限性</h3><p>RNN的关键局限：</p><ul><li><p>循环模型通常沿着输入和输出序列的符号位置进行计算</p></li><li><p>这种本质上的顺序性质阻碍了训练样例内的并行化，在更长序列长度下尤为关键</p></li><li><p>虽然一些工作通过分解技巧和条件计算提高了计算效率，但顺序计算的基本约束仍然存在</p></li></ul><h3 id="注意力机制的重要性">注意力机制的重要性</h3><ul><li><p>注意力机制已成为序列建模和转导模型的重要组成部分，允许在不考虑输入或输出序列中距离的情况下建模依赖关系</p></li><li><p>然而，大多数情况下，此类注意力机制与循环网络结合使用</p></li></ul><h3 id="transformer模型的提出">Transformer模型的提出</h3><ul><li><p>作者提出Transformer，一种完全放弃循环结构，而是完全依赖注意力机制来获取输入和输出间全局依赖关系的架构</p></li><li><p>Transformer允许更多并行化，在8个P100 GPU上训练仅12小时就达到机器翻译的新水平</p></li></ul><h3 id="相关工作比较">相关工作比较</h3><p>论文讨论了其他减少顺序计算的模型：</p><ul><li><p>Extended Neural GPU、ByteNet和ConvS2S都使用CNN作为基本构建块</p></li><li><p>这些模型中，关联两个任意输入或输出位置的操作数随距离增长(ConvS2S线性增长，ByteNet对数增长)</p></li><li><p>Transformer将此减少为常数级别的操作，尽管以注意力加权位置平均导致的有效分辨率降低为代价，但后续通过多头注意力机制进行了补偿（具体方法在3.2节）</p></li></ul><p>论文特别提到自注意力(self-attention)机制：</p><ul><li><p>用于关联单个序列中的不同位置以计算序列表示</p></li><li><p>已成功应用于阅读理解、摘要生成、文本蕴含和学习任务无关的句子表示</p></li></ul><h2 id="模型架构">模型架构</h2><h3 id="二编码器encoder细节"><strong>二、编码器（Encoder）细节</strong></h3><h4 id="1-单层结构"><strong>1. 单层结构</strong></h4><p>每层编码器包含两个子层：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入 → 多头自注意力 → 残差连接+层归一化 → 前馈网络 → 残差连接+层归一化 → 输出</span><br></pre></td></tr></table></figure><h4 id="2-关键技术解析"><strong>2. 关键技术解析</strong></h4><ul><li><strong>多头自注意力 (Multi-Head Self-Attention)</strong><ul><li><strong>作用</strong>：让每个词查看全句其他词，动态计算关联权重 例：句子"Animal didn't cross the street because it was too tired"中，"it"通过自注意力定位到"Animal"</li><li><strong>多头机制</strong>：将512维向量拆分为8个64维头，并行计算后拼接（类似8组不同滤镜观察句子）</li></ul></li><li><strong>前馈网络 (Position-wise FFN)</strong><ul><li><strong>结构</strong>：线性层→ReLU→线性层（公式：FFN(x)=max(0,xW1+b1)W2+b2）</li><li><strong>特性</strong>：对每个词独立处理，无位置交互（与自注意力互补）</li></ul></li><li><strong>残差连接 + 层归一化</strong><ul><li><strong>操作</strong>：输出 = LayerNorm(原始x + 计算后的x)</li><li><strong>目的</strong>：缓解梯度消失，加速深层网络训练</li><li><strong>维度保持</strong>：所有子层输出保持d_model=512，便于叠加</li></ul></li></ul><h4 id="3-6层堆叠的意义"><strong>3. 6层堆叠的意义</strong></h4><ul><li>底层：学习局部语法（如名词短语结构）</li><li>中层：捕捉句间逻辑关系（因果、转折）</li><li>高层：整合全局语义（篇章级信息）</li></ul><hr><h3 id="三解码器decoder细节"><strong>三、解码器（Decoder）细节</strong></h3><h4 id="1-单层结构-1"><strong>1. 单层结构</strong></h4><p>每层解码器包含三个子层：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入 → 掩码自注意力 → 残差连接+层归一化 → 编码器-解码器注意力 → 残差连接+层归一化 → 前馈网络 → 残接+层归一化 → 输出</span><br></pre></td></tr></table></figure><h4 id="2-核心差异点"><strong>2. 核心差异点</strong></h4><ul><li><strong>掩码自注意力 (Masked Self-Attention)</strong><ul><li><strong>实现方式</strong>：在softmax前将未来位置权重设为-∞</li><li><strong>作用</strong>：确保预测第t个词时仅依赖前t-1个词 例：生成"I love"时，第三个词只能基于前两个词预测</li></ul></li><li><strong>编码器-解码器注意力</strong><ul><li><strong>Query来源</strong>：解码器当前状态</li><li><strong>Key/Value来源</strong>：编码器最终输出</li><li><strong>功能</strong>：实现源语言与目标语言的词对齐 可视化案例：翻译"knowledge"时可能重点关注"知识"</li></ul></li></ul><h4 id="3-位置偏移技巧"><strong>3. 位置偏移技巧</strong></h4><ul><li><strong>输出嵌入右移一位</strong>： 训练时解码器输入是目标序列右移后的结果，确保当前位置预测仅依赖历史信息</li></ul><hr><h3 id="q---a">Q---A</h3><h4 id="q1-为什么每层都要残差连接"><strong>Q1: 为什么每层都要残差连接？</strong></h4><ul><li>深层网络信号传递容易衰减，残差结构保留原始信息（类似高速公路）</li><li>实验证明：无残差连接时6层网络难以收敛</li></ul><h4 id="q2-层归一化-vs-批归一化"><strong>Q2: 层归一化 vs 批归一化</strong></h4><ul><li>批归一化：对同一批次内所有样本的同一特征做归一化（适合CV）</li><li><strong>层归一化</strong>：对单个样本的所有特征做归一化（更适合NLP变长序列）</li></ul><h4 id="q3-为什么用6层而不是更多"><strong>Q3: 为什么用6层而不是更多？</strong></h4><ul><li>论文实验表明：6层在翻译任务上达到效率与效果的平衡 （后续研究如BERT使用12/24层，需更多数据与算力支持）</li></ul><hr><h3 id="五实例演算流程"><strong>五、实例演算流程</strong></h3><p>以英译中为例：</p><ol type="1"><li><strong>编码过程</strong> 输入："The cat sat on the mat" 经过6层编码器后，每个词被转换为包含上下文信息的512维向量</li><li><strong>解码过程</strong> 生成："猫坐在垫子上"<ul><li>第1步：接收起始符，通过掩码自注意力初始化状态</li><li>第2步：生成"猫"时，编码器-解码器注意力聚焦"cat"</li><li>第3步：生成"坐"时，自注意力关联"sat"，编码器注意力保持对"cat"关注</li><li>后续步骤同理，直至生成终止符</li></ul></li></ol><hr><p><strong>关键结论</strong>：Transformer通过<strong>堆叠自注意力层+位置感知前馈网络</strong>，实现了： ① 完全并行化计算 ② 长距离依赖捕捉 ③ 端到端对齐能力。这种设计成为后续GPT、BERT等模型的基石。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ccloire.com">Loire</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ccloire.com/2025/04/25/%E3%80%8AAttention%20is%20all%20your%20need%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">https://ccloire.com/2025/04/25/%E3%80%8AAttention%20is%20all%20your%20need%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://ccloire.com" target="_blank">Loire's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">多模态大模型论文学习</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav id="pagination"><div class="pagination"></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Loire</div><div class="author-info-description">记录CS学习路线以及各类经验分享</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/12/%E5%8D%97%E4%BA%AC4%E5%A4%A93%E5%A4%9C%E6%94%BB%E7%95%A5/" title="南京4天3夜攻略">南京4天3夜攻略</a><time datetime="2025-06-12T04:47:58.000Z" title="发表于 2025-06-12 12:47:58">2025-06-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/30/%E8%AE%A1%E7%BD%91%E5%AE%9E%E9%AA%8C-%E5%9F%BA%E6%9C%AC%E7%BD%91%E7%BB%9C%E6%8C%87%E4%BB%A4/" title="计网实验--基本网络指令">计网实验--基本网络指令</a><time datetime="2025-04-30T06:51:22.000Z" title="发表于 2025-04-30 14:51:22">2025-04-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/25/%E3%80%8AAttention%20is%20all%20your%20need%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="《Attention is all your need》论文笔记">《Attention is all your need》论文笔记</a><time datetime="2025-04-25T05:10:05.000Z" title="发表于 2025-04-25 13:10:05">2025-04-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/25/RNN%E7%AE%80%E4%BB%8B/" title="RNN简介">RNN简介</a><time datetime="2025-04-25T05:05:09.000Z" title="发表于 2025-04-25 13:05:09">2025-04-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%92%8C%E5%9B%A0%E7%89%B9%E7%BD%91/" title="计算机网络--第一章：计算机网络和因特网">计算机网络--第一章：计算机网络和因特网</a><time datetime="2025-04-08T07:11:17.000Z" title="发表于 2025-04-08 15:11:17">2025-04-08</time></div></div></div></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/" style="font-size:1.1em;color:#999">技术分享</a> <a href="/tags/%E7%94%9F%E6%B4%BB/" style="font-size:1.1em;color:#999">生活</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size:1.1em;color:#999">计算机网络</a> <a href="/tags/%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/" style="font-size:1.1em;color:#999">基础工具使用指南</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size:1.1em;color:#999">深度学习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" style="font-size:1.1em;color:#999">多模态大模型论文学习</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i> <span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/06/"><span class="card-archive-list-date">六月 2025 </span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025 </span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/03/"><span class="card-archive-list-date">三月 2025 </span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025 </span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025 </span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">9</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastpushdate="2025-06-13T16:24:14.056Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Loire</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const e=()=>{(()=>{const e=document.querySelectorAll("pre > code.mermaid");0!==e.length&&e.forEach(e=>{const t=document.createElement("pre");t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent;const n=document.createElement("div");n.className="mermaid-wrap",n.appendChild(t),e.parentNode.replaceWith(n)})})();const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>(e=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,n)=>{const d=e.firstElementChild,a="mermaid-"+n,r=`%%{init:{ 'theme':'${t}'}}%%\n`+d.textContent,m=mermaid.render(a,r),o=e=>{d.insertAdjacentHTML("afterend",e)};"string"==typeof m?o(m):m.then(({svg:e})=>o(e))})})(e);btf.addGlobalFn("themeChange",t,"mermaid"),window.loadMermaid?t():btf.getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(t)};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{const t=(e,a)=>{"object"==typeof e&&null!==e&&Object.keys(e).forEach(r=>{const n=e[r];"object"==typeof n&&null!==n&&(n[a]?e[r]=n[a]:t(n,a))})},e=e=>{window.loadChartJS=!0,Array.from(e).forEach((e,a)=>{const r=e.firstElementChild,n=e.getAttribute("data-chartjs-id")||"chartjs-"+a,d=e.getAttribute("data-width"),o=document.getElementById(n);o&&o.parentNode.remove();const c=r.textContent,l=document.createElement("canvas");l.id=n;const s=document.createElement("div");s.className="chartjs-wrap",d&&(s.style.width=d),s.appendChild(l),r.insertAdjacentElement("afterend",s);const h=document.getElementById(n).getContext("2d"),i=JSON.parse(c),m="dark"===document.documentElement.getAttribute("data-theme")?"dark-mode":"light-mode";(t=>{"dark-mode"===t?(Chart.defaults.color="rgba(255, 255, 255, 0.8)",Chart.defaults.borderColor="rgba(255, 255, 255, 0.2)",Chart.defaults.scale.ticks.backdropColor="transparent"):(Chart.defaults.color="rgba(0, 0, 0, 0.8)",Chart.defaults.borderColor="rgba(0, 0, 0, 0.1)",Chart.defaults.scale.ticks.backdropColor="transparent")})(m),t(i,m),new Chart(h,i)})},a=()=>{const t=document.querySelectorAll("#article-container .chartjs-container");0!==t.length&&(window.loadChartJS?e(t):btf.getScript("https://cdn.jsdelivr.net/npm/chart.js/dist/chart.umd.min.js").then(()=>e(t)))};btf.addGlobalFn("themeChange",a,"chartjs"),btf.addGlobalFn("encrypt",a,"chartjs"),window.pjax?a():document.addEventListener("DOMContentLoaded",a)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>