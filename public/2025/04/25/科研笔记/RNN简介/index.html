<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>RNN简介 | Loire's Blog</title><meta name="author" content="Loire"><meta name="copyright" content="Loire"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="RNN(循环神经网络)简要介绍 简介 循环神经网络模型以序列数据为输入（数据内部的元素是有顺序关系的），如文章、语句、一周的天气信息、三个月的股市指数等。与传统的前馈网络不同的是，RNN模型处理序列数据能够获取更多的语义信息，时序信息等。 处理任务示例： 以NER（命名实体识别）为例，从自然语言文本中识别真实世界中的实体名及其类别。如： 句子1：I like eating apple!—"><meta property="og:type" content="article"><meta property="og:title" content="RNN简介"><meta property="og:url" content="https://ccloire.com/2025/04/25/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/RNN%E7%AE%80%E4%BB%8B/index.html"><meta property="og:site_name" content="Loire&#39;s Blog"><meta property="og:description" content="RNN(循环神经网络)简要介绍 简介 循环神经网络模型以序列数据为输入（数据内部的元素是有顺序关系的），如文章、语句、一周的天气信息、三个月的股市指数等。与传统的前馈网络不同的是，RNN模型处理序列数据能够获取更多的语义信息，时序信息等。 处理任务示例： 以NER（命名实体识别）为例，从自然语言文本中识别真实世界中的实体名及其类别。如： 句子1：I like eating apple!—"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://ccloire.com/img/butterfly-icon.png"><meta property="article:published_time" content="2025-04-25T05:05:09.000Z"><meta property="article:modified_time" content="2025-06-13T16:35:28.480Z"><meta property="article:author" content="Loire"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://ccloire.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RNN简介",
  "url": "https://ccloire.com/2025/04/25/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/RNN%E7%AE%80%E4%BB%8B/",
  "image": "https://ccloire.com/img/butterfly-icon.png",
  "datePublished": "2025-04-25T05:05:09.000Z",
  "dateModified": "2025-06-13T16:35:28.480Z",
  "author": [
    {
      "@type": "Person",
      "name": "Loire",
      "url": "https://ccloire.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://ccloire.com/2025/04/25/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/RNN%E7%AE%80%E4%BB%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"RNN简介",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Loire's Blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章发布时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-heartbeat"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://caimotu.top/Picgo/微信图片_20250226100519.jpg" alt="Logo"><span class="site-name">Loire's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">RNN简介</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span> 返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章发布时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-heartbeat"></i><span> 清单</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">RNN简介</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-25T05:05:09.000Z" title="发表于 2025-04-25 13:05:09">2025-04-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-06-13T16:35:28.480Z" title="更新于 2025-06-14 00:35:28">2025-06-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="rnn循环神经网络简要介绍">RNN(循环神经网络)简要介绍</h1><h2 id="简介">简介</h2><p>循环神经网络模型以序列数据为输入（数据内部的元素是有顺序关系的），如文章、语句、一周的天气信息、三个月的股市指数等。与传统的前馈网络不同的是，RNN模型处理序列数据能够获取更多的语义信息，时序信息等。</p><p>处理任务示例：</p><p>以NER（命名实体识别）为例，从自然语言文本中识别真实世界中的实体名及其类别。如：</p><p>句子1：I like eating apple!——其中的apple指的是苹果食物</p><p>句子2：The Apple is a great company！——其中的Apple指的是苹果公司</p><p>而如果是传统的DNN（深度神经网络）模型，由于输入方式为逐元素输入，无法有效获取上下文信息，则若训练集中的apple一词大部分被标记为苹果食物，那么对测试集中的apple处理也将全部标记为苹果食物而非根据实际上下文推断。</p><h2 id="模型提出">模型提出</h2><h3 id="基本rnn结构">基本RNN结构</h3><p>为了解决普通DNN（深度神经网络）逐元素输入而无法有效获取上下文信息的问题。RNN最基本的改良点在于增加一个模块用于储存上下文信息，下图即是一个典型的RNN结构示意：</p><p><img src="https://caimotu.top/Picgo/image-20250417165208064.png" alt="image-20250417165208064" style="zoom:33%"></p><p>其中I（输入序列）到O（输出序列）的过程增加了一个保存上下文信息的权重矩阵W，也就是每次输出O不仅要考虑当前的输入数据I，还要考虑上一次输出的隐藏序列W（保存上下文）。RNN就是一个循环递归地处理上述输入输出的过程。</p><h3 id="rnn展开结构">RNN展开结构</h3><p>将上图的基本结构展开，就成为了下图展示的模型计算过程：</p><p><img src="https://caimotu.top/Picgo/ebe46967a3e80dab4c04635a2c6785f.jpg" alt="ebe46967a3e80dab4c04635a2c6785f" style="zoom:33%"></p><p>其中 <span class="math inline">\(x_i\)</span> 表示i时刻的模型输入，<span class="math inline">\(y_i\)</span>表示<span class="math inline">\(x_i\)</span>对应的输出结果，模型计算公式如下： <span class="math display">\[ \begin{split} y_i = g(Vh_i) \\ \\ h_i = f(Ux_i + Wh_{i-1}) \end{split} \]</span> U表示当前输入数据的权重因子，W表示决定上下文信息影响程度的参数矩阵，<span class="math inline">\(h_i\)</span>则表示当前隐藏层的输出隐变量。可以看出决定当前输出<span class="math inline">\(y_i\)</span>的隐变量<span class="math inline">\(h_i\)</span>不仅由当前输入<span class="math inline">\(x_i\)</span>决定，也与上一时刻的隐变量<span class="math inline">\(h_{i-1}\)</span>有关。（PS：整个模型计算过程里用的参数矩阵W是一定的）</p><h2 id="rnn模型结构变化">RNN模型结构变化</h2><p>上述展开结构会根据输入长度和输出长度的变化而产生不同的结构</p><h3 id="n-to-n结构">N to N结构</h3><p>这类结构的输入长度与输出长度相同，也即每一个输入值都会对应一个输出值。通常用于逐序列判断或分类任务（如序列标注，NER，视频帧分类等）。示例图与计算模型如2.2所述。</p><h3 id="n-to-1结构">N to 1结构</h3><p>这类结构只有一个输出值，表示输出结果包含了整个输入序列的语义信息和上下文信息。结构示意图：</p><p><img src="https://caimotu.top/Picgo/8606007dd425e9195f0c11d4719656f.jpg" alt="8606007dd425e9195f0c11d4719656f" style="zoom:50%"></p><p>计算模型： <span class="math display">\[ \begin{split} Y = y_N = g(Vh_N) \\ h_i = f(Ux_i+ Wh_{i-1}) \end{split} \]</span> 这类结构通常用于文字分类、文章分类以及图像分类任务。</p><h3 id="to-n结构">1 to N结构</h3><p>一个输入数据对应一系列输出，其意义是一个起始状态或种子数据会随时间变化生成一个序列的输出结果。</p><p>若输入数据只在首个时刻输入，则计算示意图如下所示：</p><p><img src="https://caimotu.top/Picgo/112ce7e4cbc3de28915ec314a82fdba.jpg" alt="112ce7e4cbc3de28915ec314a82fdba" style="zoom:50%"></p><p>而若输入数据在每个时刻都作为输入，则示意图如下：</p><p><img src="https://caimotu.top/Picgo/bd9767d63cc2fd88b9b265d80777250.jpg" alt="bd9767d63cc2fd88b9b265d80777250" style="zoom:50%"></p><p>该结构通常用于由图像自动生成文章、类别生成音乐、文章、代码等由种子数据生成序列的任务。</p><h3 id="n-to-m结构encoder-decoder模型seq2seq模型">N to M结构（encoder-decoder模型、seq2seq模型）</h3><p>最后一类是输入输出序列长度不相等的结构。通常采用一个N to 1结构和一个1 to M结构组合来实现，如下图：</p><p><img src="https://caimotu.top/Picgo/f529bf34ed1f0a34e98af6cd3b390b2.jpg" alt="f529bf34ed1f0a34e98af6cd3b390b2" style="zoom:50%"></p><p><img src="https://caimotu.top/Picgo/6355d703227ac85b3ac67a61c52bd84.jpg" alt="6355d703227ac85b3ac67a61c52bd84" style="zoom:33%"></p><p>由上图可以看出，两个不同长度的RNN模型组合可以控制输出序列长度。两个模型之间通过一个上下文向量C来链接，其中C可作为第二个RNN模型的输入数据并对初始隐藏变量<span class="math inline">\(h_0^{&#39;}\)</span>​进行初始化（如第一个图），也可以直接被用来初始化第二个RNN模型的隐藏变量（如第二个图）。</p><p>常用的上下文向量C的求解方法有： <span class="math display">\[ \begin{split} c = h_N \\ c = g(h_N) \\ c = g(h_1 :: h_N) \end{split} \]</span> 第一种方法直接将encoder的输出作为上下文向量；第二种方法则需要先对encoder输出进行变换；第三种方法则将encoder的一个输出序列进行变换，而非单一选取最后一个输出。</p><p>通常将第一个RNN模型作为encoder（编码器），第二个RNN模型成为decoder（解码器）。通过这样N to M的RNN模型，我们可以处理各类序列处理任务，如语音识别，文本摘要，机器翻译，图像描述生成等。</p><h2 id="梯度消失与梯度爆炸">梯度消失与梯度爆炸</h2><h3 id="概念">概念</h3><p>由于RNN中的上下文参数矩阵是权重共享的，即当进行梯度更新时，对该矩阵求偏导数时需要加入时序影响，将导致存在基于时序数量的权重“连乘”。若某一阶段权重值过小，结合“连乘”将导致最终权重趋于“无穷小”（即等于0），此现象称为“梯度消失”。相反地，若权重值过大，经连乘后将导致权重值变得过大，称为“梯度爆炸”。</p><p>与普通NN的梯度消失及梯度爆炸不同，RNN的梯度爆炸（或消失）是根本原因是“连乘”，是在反向传播的某一阶段出现的，在此之前的反向传播不受影响。</p><h3 id="如何解决">如何解决</h3><p><strong>1. 梯度爆炸的解决</strong></p><p>1）梯度裁剪</p><p>​ 梯度裁剪即为梯度更新时的梯度设置上限，当超过阈值将强制裁剪，避免出现过高阈值。</p><p><strong>2. 梯度消失的解决</strong></p><p>1）使用Relu激活函数</p><p>​ 使用Relu激活函数解决梯度消失的原理是，Relu函数在自变量大于0是，因变量恒为1，由此避免梯度过小。</p><p>2）变更RNN结构</p><p>​ 改用变种版本的RNN结构，常见的包括LSTM模型及GRU模型。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://ccloire.com">Loire</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://ccloire.com/2025/04/25/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/RNN%E7%AE%80%E4%BB%8B/">https://ccloire.com/2025/04/25/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/RNN%E7%AE%80%E4%BB%8B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://ccloire.com" target="_blank">Loire's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/25/%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0/%E3%80%8AAttention%20is%20all%20your%20need%E3%80%8B%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="《Attention is all your need》论文笔记"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">《Attention is all your need》论文笔记</div></div><div class="info-2"><div class="info-item-1">论文逐步分解读 论文网址： https://paperswithcode.com/paper/attention-is-all-you-need https://arxiv.org/abs/1706.03762 Transformer模型论文摘要解读 这篇论文是著名的”Attention Is All You Need”论文的摘要，介绍了Transformer模型，这是深度学习和自然语言处理领域的一个里程碑式创新。以下是主要内容解读： 研究背景与创新 之前的主流序列转导模型都基于复杂的循环神经网络(RNN)或卷积神经网络(CNN)，包含编码器和解码器结构 性能最好的模型还通过注意力机制连接编码器和解码器 论文提出了全新的网络架构”Transformer”，完全基于注意力机制，彻底摒弃了循环和卷积结构 实验结果 在两个机器翻译任务上的实验表明，Transformer模型质量更优，同时更易于并行化，训练时间显著缩短 在WMT 2014英德翻译任务上达到28.4 BLEU分，比当时最好的结果(包括集成模型)高出2个BLEU以上 在WMT 201...</div></div></div></a><a class="pagination-related" href="/2025/04/08/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%92%8C%E5%9B%A0%E7%89%B9%E7%BD%91/" title="计算机网络--第一章：计算机网络和因特网"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">计算机网络--第一章：计算机网络和因特网</div></div><div class="info-2"><div class="info-item-1">计算机网络和因特网 章节导读： 在这个章节，我们需要先了解什么是因特网（Internet），什么是协议（protocol）。 之后了解什么是网络边缘(network edge)：主机hosts，接入网access network以及物理媒介physical media。 边缘之后便是网络核心(network core)的概念：主要了解两种网络技术——分组交换packet switching以及电路交换circuit switching，这是互联网的技术基础，会涉及到包(packet）、路由器(router)和链路层交换机(link-layer switch)这些概念；以及互联网的结构。 再往下是网络性能（performance）：了解数据从源端发送到目的地时数据包是如何丢失(loss)或者延迟(delay)的。这一部分内容会将吞吐量(throughput)作为一种性能指标，衡量字节信号从源转发到目的地的速率。 之后我们需要了解互联网中的协议层(protocol layers)和服务模型(services models)相关内容 最后便是关于互联网中的...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">Loire</div><div class="author-info-description">记录CS学习路线以及各类经验分享</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">14</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#rnn%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">RNN(循环神经网络)简要介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%87%BA"><span class="toc-number">1.2.</span> <span class="toc-text">模型提出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%ACrnn%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.1.</span> <span class="toc-text">基本RNN结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E5%B1%95%E5%BC%80%E7%BB%93%E6%9E%84"><span class="toc-number">1.2.2.</span> <span class="toc-text">RNN展开结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnn%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%8F%98%E5%8C%96"><span class="toc-number">1.3.</span> <span class="toc-text">RNN模型结构变化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#n-to-n%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.1.</span> <span class="toc-text">N to N结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n-to-1%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.2.</span> <span class="toc-text">N to 1结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#to-n%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.3.</span> <span class="toc-text">1 to N结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n-to-m%E7%BB%93%E6%9E%84encoder-decoder%E6%A8%A1%E5%9E%8Bseq2seq%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.4.</span> <span class="toc-text">N to M结构（encoder-decoder模型、seq2seq模型）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">1.4.</span> <span class="toc-text">梯度消失与梯度爆炸</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">1.4.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3"><span class="toc-number">1.4.2.</span> <span class="toc-text">如何解决</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/12/C++%E5%90%88%E9%9B%86/C&amp;C++%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0/" title="C&amp;C++基础语法-命令行参数">C&amp;C++基础语法-命令行参数</a><time datetime="2025-08-12T04:53:23.000Z" title="发表于 2025-08-12 12:53:23">2025-08-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/11/C++%E5%90%88%E9%9B%86/C&amp;C++%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95-%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8F%E6%80%BB%E7%BB%93/" title="C&amp;C++基础语法--静态变量总结">C&amp;C++基础语法--静态变量总结</a><time datetime="2025-08-11T06:24:03.000Z" title="发表于 2025-08-11 14:24:03">2025-08-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/07/C++%E5%90%88%E9%9B%86/C++%E5%88%9D%E7%BA%A7%E2%80%94%E2%80%94%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E4%B8%8E%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/" title="C++初级——内存空间与编译原理">C++初级——内存空间与编译原理</a><time datetime="2025-08-07T14:13:10.000Z" title="发表于 2025-08-07 22:13:10">2025-08-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E7%BA%BF%E6%80%A7%E8%A1%A8/" title="数据结构与算法---线性表">数据结构与算法---线性表</a><time datetime="2025-07-24T15:38:42.000Z" title="发表于 2025-07-24 23:38:42">2025-07-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E7%BB%AA%E8%AE%BA/" title="数据结构与算法---算法分析绪论">数据结构与算法---算法分析绪论</a><time datetime="2025-07-23T07:57:19.000Z" title="发表于 2025-07-23 15:57:19">2025-07-23</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2019 - 2025 By Loire</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"all"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script><script>(()=>{const e=()=>{(()=>{const e=document.querySelectorAll("pre > code.mermaid");0!==e.length&&e.forEach(e=>{const t=document.createElement("pre");t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent;const n=document.createElement("div");n.className="mermaid-wrap",n.appendChild(t),e.parentNode.replaceWith(n)})})();const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>(e=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,n)=>{const d=e.firstElementChild,a="mermaid-"+n,r=`%%{init:{ 'theme':'${t}'}}%%\n`+d.textContent,m=mermaid.render(a,r),o=e=>{d.insertAdjacentHTML("afterend",e)};"string"==typeof m?o(m):m.then(({svg:e})=>o(e))})})(e);btf.addGlobalFn("themeChange",t,"mermaid"),window.loadMermaid?t():btf.getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(t)};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{const t=(e,a)=>{"object"==typeof e&&null!==e&&Object.keys(e).forEach(r=>{const n=e[r];"object"==typeof n&&null!==n&&(n[a]?e[r]=n[a]:t(n,a))})},e=e=>{window.loadChartJS=!0,Array.from(e).forEach((e,a)=>{const r=e.firstElementChild,n=e.getAttribute("data-chartjs-id")||"chartjs-"+a,d=e.getAttribute("data-width"),o=document.getElementById(n);o&&o.parentNode.remove();const c=r.textContent,l=document.createElement("canvas");l.id=n;const s=document.createElement("div");s.className="chartjs-wrap",d&&(s.style.width=d),s.appendChild(l),r.insertAdjacentElement("afterend",s);const h=document.getElementById(n).getContext("2d"),i=JSON.parse(c),m="dark"===document.documentElement.getAttribute("data-theme")?"dark-mode":"light-mode";(t=>{"dark-mode"===t?(Chart.defaults.color="rgba(255, 255, 255, 0.8)",Chart.defaults.borderColor="rgba(255, 255, 255, 0.2)",Chart.defaults.scale.ticks.backdropColor="transparent"):(Chart.defaults.color="rgba(0, 0, 0, 0.8)",Chart.defaults.borderColor="rgba(0, 0, 0, 0.1)",Chart.defaults.scale.ticks.backdropColor="transparent")})(m),t(i,m),new Chart(h,i)})},a=()=>{const t=document.querySelectorAll("#article-container .chartjs-container");0!==t.length&&(window.loadChartJS?e(t):btf.getScript("https://cdn.jsdelivr.net/npm/chart.js/dist/chart.umd.min.js").then(()=>e(t)))};btf.addGlobalFn("themeChange",a,"chartjs"),btf.addGlobalFn("encrypt",a,"chartjs"),window.pjax?a():document.addEventListener("DOMContentLoaded",a)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>