---
layout: posts
title: 《Attention is all your need》论文笔记
date: 2025-04-25 13:10:05
tags: 多模态大模型论文学习
---

# 论文逐步分解读

论文网址：

https://paperswithcode.com/paper/attention-is-all-you-need

https://arxiv.org/abs/1706.03762

## Transformer模型论文摘要解读

这篇论文是著名的"Attention Is All You Need"论文的摘要，介绍了Transformer模型，这是深度学习和自然语言处理领域的一个里程碑式创新。以下是主要内容解读：

### 研究背景与创新

- 之前的主流序列转导模型都基于复杂的循环神经网络(RNN)或卷积神经网络(CNN)，包含编码器和解码器结构

- 性能最好的模型还通过注意力机制连接编码器和解码器

- 论文提出了全新的网络架构"Transformer"，完全基于注意力机制，彻底摒弃了循环和卷积结构

### 实验结果

- 在两个机器翻译任务上的实验表明，Transformer模型质量更优，同时更易于并行化，训练时间显著缩短

- 在WMT 2014英德翻译任务上达到28.4 BLEU分，比当时最好的结果(包括集成模型)高出2个BLEU以上

- 在WMT 2014英法翻译任务上，单模型达到41.8 BLEU的最新水平，仅用8个GPU训练3.5天，训练成本只是文献中最佳模型的一小部分

### 泛化能力

- Transformer在其他任务上表现良好，成功应用于英语句法分析，无论是大规模还是有限的训练数据环境



## 研究背景

论文开篇指出：

- 循环神经网络(RNN)特别是LSTM和GRU已成为序列建模和转导问题(如语言建模和机器翻译)的最先进方法

- 许多研究不断推动循环语言模型和编码器-解码器架构的边界

### 现有方法的局限性

RNN的关键局限：

- 循环模型通常沿着输入和输出序列的符号位置进行计算

- 这种本质上的顺序性质阻碍了训练样例内的并行化，在更长序列长度下尤为关键

- 虽然一些工作通过分解技巧和条件计算提高了计算效率，但顺序计算的基本约束仍然存在

### 注意力机制的重要性

- 注意力机制已成为序列建模和转导模型的重要组成部分，允许在不考虑输入或输出序列中距离的情况下建模依赖关系

- 然而，大多数情况下，此类注意力机制与循环网络结合使用

### Transformer模型的提出

- 作者提出Transformer，一种完全放弃循环结构，而是完全依赖注意力机制来获取输入和输出间全局依赖关系的架构

- Transformer允许更多并行化，在8个P100 GPU上训练仅12小时就达到机器翻译的新水平

### 相关工作比较

论文讨论了其他减少顺序计算的模型：

- Extended Neural GPU、ByteNet和ConvS2S都使用CNN作为基本构建块

- 这些模型中，关联两个任意输入或输出位置的操作数随距离增长(ConvS2S线性增长，ByteNet对数增长)

- Transformer将此减少为常数级别的操作，尽管以注意力加权位置平均导致的有效分辨率降低为代价，但后续通过多头注意力机制进行了补偿（具体方法在3.2节）

论文特别提到自注意力(self-attention)机制：

- 用于关联单个序列中的不同位置以计算序列表示

- 已成功应用于阅读理解、摘要生成、文本蕴含和学习任务无关的句子表示



## 模型架构

### **二、编码器（Encoder）细节**

#### **1. 单层结构**

每层编码器包含两个子层：

```
输入 → 多头自注意力 → 残差连接+层归一化 → 前馈网络 → 残差连接+层归一化 → 输出
```

#### **2. 关键技术解析**

- **多头自注意力 (Multi-Head Self-Attention)**
  - **作用**：让每个词查看全句其他词，动态计算关联权重
    例：句子"Animal didn't cross the street because it was too tired"中，"it"通过自注意力定位到"Animal"
  - **多头机制**：将512维向量拆分为8个64维头，并行计算后拼接（类似8组不同滤镜观察句子）
- **前馈网络 (Position-wise FFN)**
  - **结构**：线性层→ReLU→线性层（公式：FFN(x)=max(0,xW1+b1)W2+b2）
  - **特性**：对每个词独立处理，无位置交互（与自注意力互补）
- **残差连接 + 层归一化**
  - **操作**：输出 = LayerNorm(原始x + 计算后的x)
  - **目的**：缓解梯度消失，加速深层网络训练
  - **维度保持**：所有子层输出保持d_model=512，便于叠加

#### **3. 6层堆叠的意义**

- 底层：学习局部语法（如名词短语结构）
- 中层：捕捉句间逻辑关系（因果、转折）
- 高层：整合全局语义（篇章级信息）

------

### **三、解码器（Decoder）细节**

#### **1. 单层结构**

每层解码器包含三个子层：

```
输入 → 掩码自注意力 → 残差连接+层归一化 → 编码器-解码器注意力 → 残差连接+层归一化 → 前馈网络 → 残接+层归一化 → 输出
```

#### **2. 核心差异点**

- **掩码自注意力 (Masked Self-Attention)**
  - **实现方式**：在softmax前将未来位置权重设为-∞
  - **作用**：确保预测第t个词时仅依赖前t-1个词
    例：生成"I love"时，第三个词只能基于前两个词预测
- **编码器-解码器注意力**
  - **Query来源**：解码器当前状态
  - **Key/Value来源**：编码器最终输出
  - **功能**：实现源语言与目标语言的词对齐
    可视化案例：翻译"knowledge"时可能重点关注"知识"

#### **3. 位置偏移技巧**

- **输出嵌入右移一位**：
  训练时解码器输入是目标序列右移后的结果，确保当前位置预测仅依赖历史信息

------

### Q---A

#### **Q1: 为什么每层都要残差连接？**

- 深层网络信号传递容易衰减，残差结构保留原始信息（类似高速公路）
- 实验证明：无残差连接时6层网络难以收敛

#### **Q2: 层归一化 vs 批归一化**

- 批归一化：对同一批次内所有样本的同一特征做归一化（适合CV）
- **层归一化**：对单个样本的所有特征做归一化（更适合NLP变长序列）

#### **Q3: 为什么用6层而不是更多？**

- 论文实验表明：6层在翻译任务上达到效率与效果的平衡
  （后续研究如BERT使用12/24层，需更多数据与算力支持）

------

### **五、实例演算流程**

以英译中为例：

1. **编码过程**
   输入："The cat sat on the mat"
   经过6层编码器后，每个词被转换为包含上下文信息的512维向量
2. **解码过程**
   生成："猫坐在垫子上"
   - 第1步：接收起始符，通过掩码自注意力初始化状态
   - 第2步：生成"猫"时，编码器-解码器注意力聚焦"cat"
   - 第3步：生成"坐"时，自注意力关联"sat"，编码器注意力保持对"cat"关注
   - 后续步骤同理，直至生成终止符

------

**关键结论**：Transformer通过**堆叠自注意力层+位置感知前馈网络**，实现了：
① 完全并行化计算 ② 长距离依赖捕捉 ③ 端到端对齐能力。这种设计成为后续GPT、BERT等模型的基石。
